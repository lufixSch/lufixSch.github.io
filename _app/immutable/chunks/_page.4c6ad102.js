import{S as jl,i as Gl,s as Kl,w as gt,x as Nl,y as Bl,z as Fl,A as zl,B as Hl,r as Ql,v as Vl,C as Jl,D as Dl,e as o,t as a,a as p,c as s,b as i,d as n,f as l,g as f,h as $l,j as c,k as t,n as Xl}from"./index.e23b10ae.js";import{A as Yl}from"./article.e19d21fa.js";function Zl(R){let u,E,I,b,m,h,A,C,je,O,Ge,Ke,Ee,q,Ne,Ie,v,Y,Be,Fe,Z,ze,Qe,g,Ve,Je,ee,Xe,be,M,Ye,ye,k,U,Ze,te,le,ge,et,oe,tt,we,S,lt,Ae,d,T,se,ot,st,it,H,ie,at,nt,rt,D,ae,ct,pt,ft,$,ne,ut,mt,dt,W,re,_t,ht,vt,j,ce,Lt,Et,ke,G,It,xe,K,bt,Ce,N,yt,Oe,B,wt,Pe,F,At,Re,_,pe,kt,xt,fe,Ct,Ot,z,Pt,ue,Rt,qt,me,Mt,Ut,de,St,qe,Q,Tt,Me,V,_e,Ht,Ue,J,Dt,Se,y,he,$t,Wt,P,jt,ve,Gt,Kt,Nt,Le,Bt,Te,He;return{c(){u=o("h1"),E=a("LocalCompletion"),I=p(),b=o("p"),m=a("Local LLM based code completion like Copilot."),h=p(),A=o("blockquote"),C=o("p"),je=a("This extension does not come with a built in backend for running LLMs. Instead you are able to use any existing tool that supports the OpenAI API format. Like the "),O=o("a"),Ge=a("Oobabooga WebUI"),Ke=a(" and many other"),Ee=p(),q=o("h2"),Ne=a("Features"),Ie=p(),v=o("ul"),Y=o("li"),Be=a("Inline (multi line) code completion"),Fe=p(),Z=o("li"),ze=a("Works with any OpenAI compatible API"),Qe=p(),g=o("li"),Ve=a("Save multiple API Endpoints and switch easily between them"),Je=p(),ee=o("li"),Xe=a("Reducing requests to LLMs by saving previous responses"),be=p(),M=o("h3"),Ye=a("Roadmap"),ye=p(),k=o("ul"),U=o("li"),Ze=a("Improve (optional) features to reduce LLM requests"),te=o("ul"),le=o("li"),ge=a("Increase time between keystrokes before requesting a new completion"),et=p(),oe=o("li"),tt=a("Give LLM incentive for shorter answers depending on the input"),we=p(),S=o("h2"),lt=a("Extension Settings"),Ae=p(),d=o("ul"),T=o("li"),se=o("code"),ot=a("localcompletion.active_endpoint"),st=a(": The URL of the API which is used for generating the code completion"),it=p(),H=o("li"),ie=o("code"),at=a("localcompletion.endpoints"),nt=a(": List of URL endpoints"),rt=p(),D=o("li"),ae=o("code"),ct=a("localcompletion.temperature"),pt=a(": Temperature of the LLM"),ft=p(),$=o("li"),ne=o("code"),ut=a("localcompletion.max_tokens"),mt=a(": Maximum number of tokens in the response"),dt=p(),W=o("li"),re=o("code"),_t=a("localcompletion.stop_sequences"),ht=a(": Additional stop sequences (max. 2)"),vt=p(),j=o("li"),ce=o("code"),Lt=a("localcompletion.reduce_calls"),Et=a(": Reduce API calls with various strategies (e.g. skip completion if last symbol was a letter)"),ke=p(),G=o("h2"),It=a("Known Issues"),xe=p(),K=o("p"),bt=a("The extension does not yet support a custom API key. This means it only works for APIs which do not need a key."),Ce=p(),N=o("p"),yt=a("Model switching is not supported at the moment as most local tools don’t support that property either."),Oe=p(),B=o("h2"),wt=a("Release Notes"),Pe=p(),F=o("h3"),At=a("0.0.3"),Re=p(),_=o("ul"),pe=o("li"),kt=a("Rework handling of old responses"),xt=p(),fe=o("li"),Ct=a("Only call new completion if input deviates from previous completion"),Ot=p(),z=o("li"),Pt=a("Add new Command: "),ue=o("strong"),Rt=a("Regenerate"),qt=p(),me=o("li"),Mt=a("Custom stop sequences"),Ut=p(),de=o("li"),St=a("Optionally reduce API calls (enabled by default)"),qe=p(),Q=o("h3"),Tt=a("0.0.2"),Me=p(),V=o("ul"),_e=o("li"),Ht=a("Reduce unnecessary requests"),Ue=p(),J=o("h3"),Dt=a("0.0.1"),Se=p(),y=o("ul"),he=o("li"),$t=a("Add basic completion"),Wt=p(),P=o("li"),jt=a("Save "),ve=o("code"),Gt=a("10"),Kt=a(" old responses to reduce LLM requests"),Nt=p(),Le=o("li"),Bt=a("Add some settings to configure the extension"),Te=p(),He=o("hr"),this.h()},l(e){u=s(e,"H1",{});var r=i(u);E=n(r,"LocalCompletion"),r.forEach(l),I=f(e),b=s(e,"P",{});var el=i(b);m=n(el,"Local LLM based code completion like Copilot."),el.forEach(l),h=f(e),A=s(e,"BLOCKQUOTE",{});var tl=i(A);C=s(tl,"P",{});var De=i(C);je=n(De,"This extension does not come with a built in backend for running LLMs. Instead you are able to use any existing tool that supports the OpenAI API format. Like the "),O=s(De,"A",{href:!0,rel:!0});var ll=i(O);Ge=n(ll,"Oobabooga WebUI"),ll.forEach(l),Ke=n(De," and many other"),De.forEach(l),tl.forEach(l),Ee=f(e),q=s(e,"H2",{});var ol=i(q);Ne=n(ol,"Features"),ol.forEach(l),Ie=f(e),v=s(e,"UL",{});var x=i(v);Y=s(x,"LI",{});var sl=i(Y);Be=n(sl,"Inline (multi line) code completion"),sl.forEach(l),Fe=f(x),Z=s(x,"LI",{});var il=i(Z);ze=n(il,"Works with any OpenAI compatible API"),il.forEach(l),Qe=f(x),g=s(x,"LI",{});var al=i(g);Ve=n(al,"Save multiple API Endpoints and switch easily between them"),al.forEach(l),Je=f(x),ee=s(x,"LI",{});var nl=i(ee);Xe=n(nl,"Reducing requests to LLMs by saving previous responses"),nl.forEach(l),x.forEach(l),be=f(e),M=s(e,"H3",{});var rl=i(M);Ye=n(rl,"Roadmap"),rl.forEach(l),ye=f(e),k=s(e,"UL",{});var $e=i(k);U=s($e,"LI",{});var Ft=i(U);Ze=n(Ft,"Improve (optional) features to reduce LLM requests"),te=s(Ft,"UL",{});var cl=i(te);le=s(cl,"LI",{});var pl=i(le);ge=n(pl,"Increase time between keystrokes before requesting a new completion"),pl.forEach(l),cl.forEach(l),Ft.forEach(l),et=f($e),oe=s($e,"LI",{});var fl=i(oe);tt=n(fl,"Give LLM incentive for shorter answers depending on the input"),fl.forEach(l),$e.forEach(l),we=f(e),S=s(e,"H2",{});var ul=i(S);lt=n(ul,"Extension Settings"),ul.forEach(l),Ae=f(e),d=s(e,"UL",{});var L=i(d);T=s(L,"LI",{});var zt=i(T);se=s(zt,"CODE",{});var ml=i(se);ot=n(ml,"localcompletion.active_endpoint"),ml.forEach(l),st=n(zt,": The URL of the API which is used for generating the code completion"),zt.forEach(l),it=f(L),H=s(L,"LI",{});var Qt=i(H);ie=s(Qt,"CODE",{});var dl=i(ie);at=n(dl,"localcompletion.endpoints"),dl.forEach(l),nt=n(Qt,": List of URL endpoints"),Qt.forEach(l),rt=f(L),D=s(L,"LI",{});var Vt=i(D);ae=s(Vt,"CODE",{});var _l=i(ae);ct=n(_l,"localcompletion.temperature"),_l.forEach(l),pt=n(Vt,": Temperature of the LLM"),Vt.forEach(l),ft=f(L),$=s(L,"LI",{});var Jt=i($);ne=s(Jt,"CODE",{});var hl=i(ne);ut=n(hl,"localcompletion.max_tokens"),hl.forEach(l),mt=n(Jt,": Maximum number of tokens in the response"),Jt.forEach(l),dt=f(L),W=s(L,"LI",{});var Xt=i(W);re=s(Xt,"CODE",{});var vl=i(re);_t=n(vl,"localcompletion.stop_sequences"),vl.forEach(l),ht=n(Xt,": Additional stop sequences (max. 2)"),Xt.forEach(l),vt=f(L),j=s(L,"LI",{});var Yt=i(j);ce=s(Yt,"CODE",{});var Ll=i(ce);Lt=n(Ll,"localcompletion.reduce_calls"),Ll.forEach(l),Et=n(Yt,": Reduce API calls with various strategies (e.g. skip completion if last symbol was a letter)"),Yt.forEach(l),L.forEach(l),ke=f(e),G=s(e,"H2",{});var El=i(G);It=n(El,"Known Issues"),El.forEach(l),xe=f(e),K=s(e,"P",{});var Il=i(K);bt=n(Il,"The extension does not yet support a custom API key. This means it only works for APIs which do not need a key."),Il.forEach(l),Ce=f(e),N=s(e,"P",{});var bl=i(N);yt=n(bl,"Model switching is not supported at the moment as most local tools don’t support that property either."),bl.forEach(l),Oe=f(e),B=s(e,"H2",{});var yl=i(B);wt=n(yl,"Release Notes"),yl.forEach(l),Pe=f(e),F=s(e,"H3",{});var wl=i(F);At=n(wl,"0.0.3"),wl.forEach(l),Re=f(e),_=s(e,"UL",{});var w=i(_);pe=s(w,"LI",{});var Al=i(pe);kt=n(Al,"Rework handling of old responses"),Al.forEach(l),xt=f(w),fe=s(w,"LI",{});var kl=i(fe);Ct=n(kl,"Only call new completion if input deviates from previous completion"),kl.forEach(l),Ot=f(w),z=s(w,"LI",{});var Zt=i(z);Pt=n(Zt,"Add new Command: "),ue=s(Zt,"STRONG",{});var xl=i(ue);Rt=n(xl,"Regenerate"),xl.forEach(l),Zt.forEach(l),qt=f(w),me=s(w,"LI",{});var Cl=i(me);Mt=n(Cl,"Custom stop sequences"),Cl.forEach(l),Ut=f(w),de=s(w,"LI",{});var Ol=i(de);St=n(Ol,"Optionally reduce API calls (enabled by default)"),Ol.forEach(l),w.forEach(l),qe=f(e),Q=s(e,"H3",{});var Pl=i(Q);Tt=n(Pl,"0.0.2"),Pl.forEach(l),Me=f(e),V=s(e,"UL",{});var Rl=i(V);_e=s(Rl,"LI",{});var ql=i(_e);Ht=n(ql,"Reduce unnecessary requests"),ql.forEach(l),Rl.forEach(l),Ue=f(e),J=s(e,"H3",{});var Ml=i(J);Dt=n(Ml,"0.0.1"),Ml.forEach(l),Se=f(e),y=s(e,"UL",{});var X=i(y);he=s(X,"LI",{});var Ul=i(he);$t=n(Ul,"Add basic completion"),Ul.forEach(l),Wt=f(X),P=s(X,"LI",{});var We=i(P);jt=n(We,"Save "),ve=s(We,"CODE",{});var Sl=i(ve);Gt=n(Sl,"10"),Sl.forEach(l),Kt=n(We," old responses to reduce LLM requests"),We.forEach(l),Nt=f(X),Le=s(X,"LI",{});var Tl=i(Le);Bt=n(Tl,"Add some settings to configure the extension"),Tl.forEach(l),X.forEach(l),Te=f(e),He=s(e,"HR",{}),this.h()},h(){$l(O,"href","https://github.com/oobabooga/text-generation-webui"),$l(O,"rel","nofollow")},m(e,r){c(e,u,r),t(u,E),c(e,I,r),c(e,b,r),t(b,m),c(e,h,r),c(e,A,r),t(A,C),t(C,je),t(C,O),t(O,Ge),t(C,Ke),c(e,Ee,r),c(e,q,r),t(q,Ne),c(e,Ie,r),c(e,v,r),t(v,Y),t(Y,Be),t(v,Fe),t(v,Z),t(Z,ze),t(v,Qe),t(v,g),t(g,Ve),t(v,Je),t(v,ee),t(ee,Xe),c(e,be,r),c(e,M,r),t(M,Ye),c(e,ye,r),c(e,k,r),t(k,U),t(U,Ze),t(U,te),t(te,le),t(le,ge),t(k,et),t(k,oe),t(oe,tt),c(e,we,r),c(e,S,r),t(S,lt),c(e,Ae,r),c(e,d,r),t(d,T),t(T,se),t(se,ot),t(T,st),t(d,it),t(d,H),t(H,ie),t(ie,at),t(H,nt),t(d,rt),t(d,D),t(D,ae),t(ae,ct),t(D,pt),t(d,ft),t(d,$),t($,ne),t(ne,ut),t($,mt),t(d,dt),t(d,W),t(W,re),t(re,_t),t(W,ht),t(d,vt),t(d,j),t(j,ce),t(ce,Lt),t(j,Et),c(e,ke,r),c(e,G,r),t(G,It),c(e,xe,r),c(e,K,r),t(K,bt),c(e,Ce,r),c(e,N,r),t(N,yt),c(e,Oe,r),c(e,B,r),t(B,wt),c(e,Pe,r),c(e,F,r),t(F,At),c(e,Re,r),c(e,_,r),t(_,pe),t(pe,kt),t(_,xt),t(_,fe),t(fe,Ct),t(_,Ot),t(_,z),t(z,Pt),t(z,ue),t(ue,Rt),t(_,qt),t(_,me),t(me,Mt),t(_,Ut),t(_,de),t(de,St),c(e,qe,r),c(e,Q,r),t(Q,Tt),c(e,Me,r),c(e,V,r),t(V,_e),t(_e,Ht),c(e,Ue,r),c(e,J,r),t(J,Dt),c(e,Se,r),c(e,y,r),t(y,he),t(he,$t),t(y,Wt),t(y,P),t(P,jt),t(P,ve),t(ve,Gt),t(P,Kt),t(y,Nt),t(y,Le),t(Le,Bt),c(e,Te,r),c(e,He,r)},p:Xl,d(e){e&&l(u),e&&l(I),e&&l(b),e&&l(h),e&&l(A),e&&l(Ee),e&&l(q),e&&l(Ie),e&&l(v),e&&l(be),e&&l(M),e&&l(ye),e&&l(k),e&&l(we),e&&l(S),e&&l(Ae),e&&l(d),e&&l(ke),e&&l(G),e&&l(xe),e&&l(K),e&&l(Ce),e&&l(N),e&&l(Oe),e&&l(B),e&&l(Pe),e&&l(F),e&&l(Re),e&&l(_),e&&l(qe),e&&l(Q),e&&l(Me),e&&l(V),e&&l(Ue),e&&l(J),e&&l(Se),e&&l(y),e&&l(Te),e&&l(He)}}}function gl(R){let u,E;const I=[R[0],Wl];let b={$$slots:{default:[Zl]},$$scope:{ctx:R}};for(let m=0;m<I.length;m+=1)b=gt(b,I[m]);return u=new Yl({props:b}),{c(){Nl(u.$$.fragment)},l(m){Bl(u.$$.fragment,m)},m(m,h){Fl(u,m,h),E=!0},p(m,[h]){const A=h&1?zl(I,[h&1&&Hl(m[0]),h&0&&Hl(Wl)]):{};h&2&&(A.$$scope={dirty:h,ctx:m}),u.$set(A)},i(m){E||(Ql(u.$$.fragment,m),E=!0)},o(m){Vl(u.$$.fragment,m),E=!1},d(m){Jl(u,m)}}}const Wl={description:"A VS Code extension for local, inline LLM based code completion using OpenAI compatible APIs like Oobabooga WebUI",image:"/articles/pcb_buttons.png",layout:"project",repository:"https://github.com/lufixSch/LocalCompletion",repository_icon:"github",status:"wip",title:"LocalCompletion",update:"https://raw.githubusercontent.com/lufixSch/LocalCompletion/main/README.md"};function eo(R,u,E){return R.$$set=I=>{E(0,u=gt(gt({},u),Dl(I)))},u=Dl(u),[u]}class oo extends jl{constructor(u){super(),Gl(this,u,eo,gl,Kl,{})}}export{oo as default,Wl as metadata};
